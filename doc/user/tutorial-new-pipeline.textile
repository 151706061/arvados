---
layout: default
navsection: userguide
title: "Tutorial: Construct a pipeline"
navorder: 15
---

h1. Tutorial: Construct a pipeline

A pipeline in Arvados is a sequence of crunch scripts, in which the output from the previous script is fed in as the input to the next script.

*This tutorial assumes that you are "logged into an Arvados VM instance":ssh-access.html#login, and have a "working environment.":check-environment.html*

h2. Create a new script

Our second script will filter the output of @parallel_hash.py@ and only include hashes that start with 0.  Create a new script in @crunch_scripts/@ called @0-filter.py@:

<pre><code class="userinput">{% include 0-filter.py %}</code></pre>

Now add it to git:

<notextile>
<pre><code>$ <span class="userinput">git add 0-filter.py</span>
$ <span class="userinput">git commit -m"zero filter"</span>
$ <span class="userinput">git push origin master</span>
</code></pre>
</notextile>

h2. Create a pipeline template

Next, create a file that contains the pipeline definition:

<notextile>
<pre><code>$ <span class="userinput">cat &gt;the_pipeline &lt;&lt;EOF
{
  "name":"my_first_pipeline",
  "components":{
    "do_hash":{
      "script":"parallel-hash.py",
      "script_parameters":{
        "input": "887cd41e9c613463eab2f0d885c6dd96+83"
      },
      "script_version":"tetron:master"
    },
    "filter":{
      "script":"0-filter.py",
      "script_parameters":{
        "input":{
          "output_of":"do_hash"
        }
      },
      "script_version":"tetron:master"
    }
  }
}
EOF
</code></pre>
</notextile>

* @"name"@ is a human-readable name for the pipeline
* @"components"@ is a set of scripts that make up the pipeline
* Each component is listed with a human-readable name (@"do_hash"@ and @"filter"@ in this example)
* Each item in @"components"@ is a single Arvados job, and uses the same format that we saw previously with @arv job create@
* @"output_of"@ indicates that the @"input"@ of @"filter"@ is the @"output"@ of the @"do_hash"@ component

The @"output_of"@ specifies a _dependency_.  Arvados uses the dependencies between jobs to automatically determine the correct order to run the jobs.

Now, use @arv pipeline_template create@ tell Arvados about your pipeline template:

<notextile>
<pre><code>$ <span class="userinput">arv pipeline_template create --pipeline-template "$(cat the_pipeline)"</span>
qr1hi-p5p6p-xxxxxxxxxxxxxxx
</code></pre>
</notextile>

Your new pipeline template will appear on the Workbench %(rarr)&rarr;% Compute %(rarr)&rarr;% Pipeline&nbsp;templates page.

h3. Running a pipeline

Run the pipeline using @arv pipeline run@, using the UUID that you received from @arv pipeline create@:

<notextile>
<pre><code>$ <span class="userinput">arv pipeline run --template qr1hi-p5p6p-xxxxxxxxxxxxxxx</span>
2013-12-16 14:08:40 +0000 -- pipeline_instance qr1hi-d1hrv-vxzkp38nlde9yyr
do_hash qr1hi-8i9sb-hoyc2u964ecv1s6 queued 2013-12-16T14:08:40Z
filter  -                           -
2013-12-16 14:08:51 +0000 -- pipeline_instance qr1hi-d1hrv-vxzkp38nlde9yyr
do_hash qr1hi-8i9sb-hoyc2u964ecv1s6 e2ccd204bca37c77c0ba59fc470cd0f7+162+K@qr1hi
filter  qr1hi-8i9sb-w5k40fztqgg9i2x queued 2013-12-16T14:08:50Z
2013-12-16 14:09:01 +0000 -- pipeline_instance qr1hi-d1hrv-vxzkp38nlde9yyr
do_hash qr1hi-8i9sb-hoyc2u964ecv1s6 e2ccd204bca37c77c0ba59fc470cd0f7+162+K@qr1hi
filter  qr1hi-8i9sb-w5k40fztqgg9i2x 735ac35adf430126cf836547731f3af6+56
</code></pre>
</notextile>

This instantiates your pipeline and displays a live feed of its status.  The new pipeline instance will also show up on the Workbench %(rarr)&rarr;% Compute %(rarr)&rarr;% Pipeline&nbsp;instances page.

Arvados adds each pipeline component to the job queue as its dependencies are satisfied (or immediately if it has no dependencies) and finishes when all components are completed or failed and there is no more work left to do.

The Keep locators of the output of each of @"do_hash"@ and @"filter"@ component are available from the output log shown above.  The output is also available on the Workbench by navigating to %(rarr)&rarr;% Compute %(rarr)&rarr;% Pipeline&nbsp;instances %(rarr)&rarr;% pipeline uuid under the *id* column %(rarr)&rarr;% components.

<notextile>
<pre><code>
$ <span class="userinput">arv keep get e2ccd204bca37c77c0ba59fc470cd0f7+162+K@qr1hi/md5sum.txt</span>
0f1d6bcf55c34bed7f92a805d2d89bbf alice.txt
504938460ef369cd275e4ef58994cffe bob.txt
8f3b36aff310e06f3c5b9e95678ff77a carol.txt
$ <span class="userinput">arv keep get 735ac35adf430126cf836547731f3af6+56</span>
0f1d6bcf55c34bed7f92a805d2d89bbf alice.txt
</code></pre>
</notextile>

Indeed, the filter has picked out just the "alice" file as having a hash that starts with 0.

h3. Running a pipeline with different parameters

Notice that the pipeline definition explicitly specifies the Keep locator for the input:

<notextile>
<pre><code>
...
    "do_hash":{
      "script_parameters":{
        "input": "887cd41e9c613463eab2f0d885c6dd96+83"
      },
    }
...
</code></pre>
</notextile>

What if we want to run the pipeline on a different input block?  One option is to define a new pipeline template, but would potentially result in clutter with many pipeline templates defined for one-off jobs.  Instead, you can override values in the input of the component like this:

<pre><code>
$ <span class="userinput">arv pipeline run --template qr1hi-p5p6p-uf9gi9nolgakm85 do_hash::input=33a9f3842b01ea3fdf27cc582f5ea2af
</code></pre>
</notextile>

<notextile>
<pre><code>
$ <span class="userinput">arv keep get 880b55fb4470b148a447ff38cacdd952+54+K@qr1hi/md5sum.txt</span>
44b8ae3fde7a8a88d2f7ebd237625b4f var-GS000016015-ASM.tsv.bz2
$ <span class="userinput">arv keep get fb728f0ffe152058fa64b9aeed344cb5+54</span>
</code></pre>
</notextile>

Since the hash of @var-GS000016015-ASM.tsv.bz2@ does not start with 0, the filter script has no output in this pipeline instance.
